{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Optimizer\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hessian(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, momentum = 0.9):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= momentum:\n",
    "            raise ValueError(\"Invalid momentum parameter: {}\".format(momentum))\n",
    "        self.iteration = -1\n",
    "        defaults = dict(lr=lr, momentum = momentum)\n",
    "        super(hessian, self).__init__(params, defaults)\n",
    "        #initialize variables \n",
    "        for group in self.param_groups:\n",
    "             for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['prev_param'] = torch.zeros_like(p)\n",
    "                state['prev_g'] = torch.zeros_like(p)\n",
    "                state['current_param'] = torch.zeros_like(p)\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        self.iteration += 1\n",
    "        for group in self.param_groups:\n",
    "            momentum = group['momentum']\n",
    "            lr = group['lr']\n",
    "            vector = []\n",
    "            grads = []\n",
    "            param = []\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                prev_param, prev_g, current_param = state['prev_param'], state['prev_g'], state['current_param']\n",
    "                #print(state)\n",
    "                with torch.no_grad():\n",
    "                    if(self.iteration == 0):\n",
    "                        prev_g.add_(p.grad)\n",
    "                        prev_param.add_(p)\n",
    "                        g_norm = torch.div(torch.norm(prev_g), prev_g)\n",
    "                        current_param.add_(prev_param.add(g_norm, alpha = -group['lr']))\n",
    "                        p.add_(g_norm, alpha = -group['lr'])\n",
    "                        param.append(p)\n",
    "                    else:\n",
    "                        vector.append(current_param.add(prev_param, alpha = -1))\n",
    "                        grads.append(p.grad)\n",
    "                        param.append(p)\n",
    "                    \n",
    "            if(self.iteration > 0):\n",
    "                dot_product = sum([(g * v).sum() for g, v in zip(grads, vector)])\n",
    "                hvp = torch.autograd.grad(dot_product, param)\n",
    "                with torch.no_grad():\n",
    "                    i = 0\n",
    "                    for p in group['params']:\n",
    "                        state = self.state[p]\n",
    "                        prev_param, prev_g, current_param = state['prev_param'], state['prev_g'], state['current_param']\n",
    "                        prev_g.add_(hvp[i]).mul_(1-group['momentum']).add_(p.grad, alpha = group['momentum'])\n",
    "                        prev_param.copy_(current_param)\n",
    "                        g_norm = torch.div(torch.norm(prev_g), prev_g)\n",
    "                        current_param.add_(g_norm, alpha = -group['lr'])\n",
    "                        p.add_(g_norm, alpha = -group['lr'])\n",
    "                        i += 1\n",
    "                    \n",
    "               \n",
    "        return loss\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron(\n",
      "  (fc1): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "import torch.nn as nn\n",
    "class Neuron(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Neuron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        out1 = self.fc1(x)\n",
    "        out2 = self.sig(out1)\n",
    "        return out2\n",
    "x = torch.tensor([-1.0, -2.0, 1.0], requires_grad = True)\n",
    "net = Neuron(3,1)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8038, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.2, 1], [0.2, 1.4], [0.5, 0.5], \n",
    "                  [-1.5, -1.3], [0.2, -1.4], [-0.7, -0.5]])\n",
    "y = torch.tensor([0, 0, 0, 1, 1, 1 ])\n",
    "\n",
    "my_neuron = Neuron(2,1)\n",
    "optimizer1 = hessian(my_neuron.parameters())\n",
    "optimizer2 = torch.optim.SGD(my_neuron.parameters(), lr = 0.001, momentum = 0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "total_loss = []\n",
    "for i in range(20):\n",
    "    optimizer1.zero_grad()\n",
    "    out = my_neuron(x)\n",
    "    loss = criterion(torch.cat((out, 1-out), axis=1), y)\n",
    "    loss.backward(create_graph = True)\n",
    "    para_list = [x for x in my_neuron.parameters()]\n",
    "    optimizer1.step()\n",
    "print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
